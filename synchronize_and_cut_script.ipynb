{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ac1994f-8e12-42da-92fb-652e9a986cc7",
   "metadata": {},
   "source": [
    "# **Aligning Video with LSL-Synced Audio & Cutting Trial Segments**\n",
    "#### *Author: Hamza Nalbantoğlu$^{1,2}$, Šárka Kadavá$^{1}$*  \n",
    "#### *Affiliation: **1 -** Leibniz-Centre General Linguistics (ZAS); **2 -** University of Potsdam (UP)*  \n",
    "#### *Contact: nalbantogluhamza@gmail.com*  \n",
    "---\n",
    "\n",
    "## **Overview**\n",
    "\n",
    "This module demonstrates a pipeline for aligning an **external video with audio** with **Lab Streaming Layer (LSL)-synced audio** of the same event. While LSL provides precise timing for recorded data streams, but some data can be still recorded outside of LSL, for instance for back-up purposes. Imagine that you recorded your video stream and audio stream via LSL, but you also decided to place to the lab one extra camera with microphone that essentially records the same experimental event. While LSL audio and video are inherently synchronized, this external camera runs on an independent timeline. However, we can use the LSL-synced audio and the audio of the external camera to align the external video with the LSL timeline.\n",
    "\n",
    "Here, we will use [**Shign**](https://github.com/KnurpsBram/shign)—a Python package designed for precise audio alignment—to synchronize the external video's audio with the LSL-synced audio. Instead of relying on manual adjustments, Shign detects timing mismatches between the two audio streams and computes the necessary **time shift** to align them. Once the correct time shift is determined, the external video will be trimmed to ensure synchronization with the LSL timeline.\n",
    "\n",
    "If you also collect timestamps to cut your LSL into individual trials (e.g., via buttonbox) you can also use these timestamps with the external video, once it is synchronized with the LSL time. We demonstrate how to compute trial start and end times for the video based on timeline of the LSL-synced audio stream. Lastly, we also show how to concatenate the external video with the LSL audio, in case you want to 1) check that the external video is indeed synchronized with the LSL data, 2) you want to replace the low-quality audio of the external video with the high-quality LSL audio.\n",
    "\n",
    "This script provides an **automated and accurate approach** to video-audio alignment, making it particularly useful for behavioural data analysis, experimental research, and other applications requiring precise synchronization.\n",
    "\n",
    "---\n",
    "## **Workflow**\n",
    "### **1. Preprocessing**\n",
    "- Extracting audio from the external video.\n",
    "- Converting the **raw LSL-synced audio** stored in a CSV file into a WAV format.\n",
    "\n",
    "### **2. Audio Alignment**\n",
    "- Using **Shign** package to align the LSL-synced audio with the extracted video audio.\n",
    "- Computing the **time shift** needed to synchronize them (shift_ms).\n",
    "\n",
    "### **3. Video Adjustment**\n",
    "- Trimming the **external video** based on the computed **time shift**.\n",
    "- Saving the **aligned video**.\n",
    "\n",
    "### **4. Trial Segmentation**\n",
    "- Extracting and computing the **trial start & end times** for the video from LSL timeline of the audio.\n",
    "- Segmenting the **aligned video** into trial-sized clips (without audio) using these timestamps.\n",
    "\n",
    "### **5. (Optional) Audio Overlay**\n",
    "- Overlaying the **corresponding trial audio** files onto each trial video to check synchronization, if available.\n",
    "\n",
    "---\n",
    "## **GitHub Repository & Installation**\n",
    "To reproduce this notebook, follow these steps:\n",
    "\n",
    "```bash\n",
    "# 1 - Clone the Repository\n",
    "git clone https://github.com/hamzanalbantoglu/lsl_audio_video_alignment.git\n",
    "cd lsl_audio_video_alignment\n",
    "\n",
    "# 2 - Create a Conda Environment (Recommended)\n",
    "conda create --name lsl_env python=3.12\n",
    "conda activate lsl_env\n",
    "\n",
    "# 3 - Install Dependencies\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# 4 - Add Conda Environment to Jupyter Notebook\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --user --name=lsl_env --display-name \"Python (lsl_env)\"\n",
    "\n",
    "# 5 - Run the Jupyter Notebook\n",
    "jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fde086-70ef-4f13-81a7-c016f6503f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa\n",
    "import subprocess\n",
    "from IPython.display import Audio, HTML\n",
    "import shign\n",
    "from shign.shign import ms_to_samples\n",
    "from scipy.io.wavfile import write"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a57ccb0-a1a2-4938-abb6-9f57372a9f23",
   "metadata": {},
   "source": [
    "## Loading Input Data & Defining Output Paths\n",
    "\n",
    "Before processing, we first specify the paths to the **external video** and the **raw LSL-synced audio**. These will serve as our primary input files to synchronize.\n",
    "\n",
    "Additionally, we define paths for storing processed audio files at different stages of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a504689-f3f0-425a-8d2c-549c5c0f62e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths:\n",
    "video_file = \"external_video/external_video.mp4\"                # External video to be aligned\n",
    "csv_file = \"lsl_synced_audio/lsl_synced_long_audio_raw.csv\"     # Raw LSL-synced audio in CSV format\n",
    "\n",
    "# Output paths:\n",
    "extracted_video_audio = \"outputs/extracted_video_audio.wav\"     # Audio extracted from the video\n",
    "lsl_synced_audio = \"outputs/lsl_synced_audio.wav\"               # LSL-synced audio converted to WAV format\n",
    "aligned_video_audio = \"outputs/aligned_video_audio.wav\"         # Aligned version of the extracted video audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4f8995-dddd-45a7-b0bc-8934d1d0fb58",
   "metadata": {},
   "source": [
    "## Inspecting the External Video for Audio Sample Rate\n",
    "\n",
    "The **sample rate** of the external video's audio track is a crucial information for two reasons:\n",
    "\n",
    "- To extract the audio while preserving its original quality.\n",
    "- To align it with the LSL-synced audio using Shign, which requires both audio tracks to have the same sample rate.\n",
    "\n",
    "FFmpeg retrieves metadata for both the video and audio streams. From the output, we can confirm that the audio sample rate is **44.1 kHz** (44100 Hz), which will be used in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89949e3-c70a-416e-bd44-9de3a5c0e62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the metadata of the video using FFmpeg:\n",
    "result = subprocess.run(\n",
    "    [\"ffmpeg\",\n",
    "     \"-hide_banner\",\n",
    "     \"-i\", video_file],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4815f2ae-c3b7-482a-b636-a065cc304367",
   "metadata": {},
   "source": [
    "## Extracting the Audio from the External Video\n",
    "\n",
    "We extract the audio track from the external video using FFmpeg, making sure that it is formatted correctly for aligning with the LSL-synced audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238eb38d-4648-4af1-89bb-bc11e5a3a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting audio from the video:\n",
    "subprocess.run([\n",
    "    \"ffmpeg\",              \n",
    "    \"-i\", video_file,           # Input video  \n",
    "    \"-vn\",                      # Remove video stream, keep only audio stream\n",
    "    \"-acodec\", \"pcm_s16le\",     # Use PCM 16-bit encoding\n",
    "    \"-ar\", \"44100\",             # Set sample rate to 44.1 kHz (checked earlier)\n",
    "    \"-ac\", \"1\",                 # Convert stereo to mono for consistency with the lsl_synced_audio\n",
    "    extracted_video_audio       \n",
    "], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e28a7c-a38b-436f-bb53-b20ef0fab495",
   "metadata": {},
   "source": [
    "## Converting the Raw LSL-Synced Audio from CSV to WAV\n",
    "\n",
    "The LSL-synced audio is stored as raw sound pressure values in a CSV file. To use it for synchronization, we need to convert it to a WAV file with the correct sample rate.\n",
    "\n",
    "- The first column of the file contains **LSL timestamps** (in milliseconds).\n",
    "- The second column contains audio **amplitude values**.\n",
    "\n",
    "We know the recording sample rate is **16 kHz** (16000 Hz). Now we extract the pressure values and save them as a WAV file using **SciPy’s** ```write()``` **function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c9653-d623-4a06-ad84-013545e90e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the CSV file and extracting the amplitude values:\n",
    "audio_data = pd.read_csv(csv_file)\n",
    "pressure = audio_data[\"1\"].values.astype(np.int16)\n",
    "\n",
    "# Saving it as a WAV file with a sample rate of 16 kHz:\n",
    "sample_rate = 16000\n",
    "write(lsl_synced_audio, sample_rate, pressure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c4ed55-204f-4dbe-b4ce-5d480bff92b1",
   "metadata": {},
   "source": [
    "## Aligning the Two Audio Tracks Using \"shign\"\n",
    "\n",
    "To synchronize the extracted_video_audio with the LSL-synced audio, we use the **```shift_align()```** function from the **Shign package**. It compares the two audio tracks using correlation, detects the timing mismatch between them, and align them based on the detected mismatch.\n",
    "\n",
    "Since the LSL audio has a sample rate of 16 kHz, while the extracted video audio has 44.1 kHz, we must **downsample the video audio** to the same sample rate before alignment. This first step is crucial to get more accurate results from the ```shift_align()``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79515d6-9b06-48b5-ac29-c3b457b22bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the LSL-synced and extracted video audio:\n",
    "lsl_audio, sr_lsl = librosa.load(lsl_synced_audio, sr=None)\n",
    "video_audio, sr_ext = librosa.load(extracted_video_audio, sr=None)\n",
    "\n",
    "# Downsampling the extracted video audio to match the LSL audio sample rate:\n",
    "video_audio_downsampled = librosa.resample(video_audio, orig_sr=sr_ext, target_sr=sr_lsl)\n",
    "sr_ext_downsampled = sr_lsl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875c7305-0b9e-4a8b-b639-153cb91356fa",
   "metadata": {},
   "source": [
    "Before aligning, we can plot both audio waveforms to inspect their initial timing differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad7d056-a9d5-49be-a6d0-008e146a8ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lsl_audio, label='lsl_audio')\n",
    "plt.plot(video_audio_downsampled, label='video_audio')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2cb621-8429-48f4-93a8-23d5a840ffa9",
   "metadata": {},
   "source": [
    "We use the modified **```shift_align()```** function from **Shign** to determine the **time shift** (in milliseconds) needed to align the video audio with the LSL audio.\n",
    "\n",
    "By default, shift_align() returns the aligned versions of both audio tracks. However, we also extract **shift_ms**, which tells us how much the video needs to be adjusted to synchronize with the LSL timeline.\n",
    "\n",
    "Key Parameters:\n",
    "- **```audio_a```** → The LSL-synced audio (reference audio).\n",
    "- **```audio_b```** → The extracted video audio (to be aligned).\n",
    "- **```sr_a```** & **```sr_b```** → Sample rates of both tracks (must match).\n",
    "- **```align_how```** → **```\"pad_and_crop_one_to_match_other\"```** ensures that only the second audio (video audio) is adjusted, keeping the LSL audio unchanged.\n",
    "- **```max_shift_sec```** = 300 → Sets a limit of 300 seconds for possible shifts (can be adjusted if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6cf31e-c00e-4f9b-a384-5ea86309dc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aligning the video audio with the LSL-synced audio:\n",
    "_, video_audio_aligned, shift_ms = shign.shift_align(  # saving shift_ms here for later use\n",
    "    audio_a = lsl_audio,\n",
    "    audio_b = video_audio_downsampled,\n",
    "    sr_a    = sr_lsl,\n",
    "    sr_b    = sr_ext_downsampled,\n",
    "    align_how = \"pad_and_crop_one_to_match_other\",\n",
    "    max_shift_sec = 300\n",
    ")\n",
    "\n",
    "print(f\"Mismatch between two audio track is \\033[1m{shift_ms/1000} seconds\\033[0m.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddea8833-db91-46cd-8e9f-a1640ab241ef",
   "metadata": {},
   "source": [
    "---\n",
    "Negative mismatch time means the second audio (video audio) starts later than the first one (LSL audio).\n",
    "\n",
    "Now, we can plot both audio signals again to verify that the extracted video audio is correctly shifted and aligned with the LSL-synced audio.\n",
    "\n",
    "We can also print the lengths of both audio arrays and play them for further verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e856e41d-f131-4bdf-88b3-286f581b036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lsl_audio, label='lsl_audio')\n",
    "plt.plot(video_audio_aligned, label='video_audio_aligned')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()\n",
    "\n",
    "# Printing the lengths of both audio arrays for verification:\n",
    "print(f\"Length of LSL audio: {len(lsl_audio)} samples\")\n",
    "print(f\"Length of aligned video audio: {len(video_audio_aligned)} samples\\n\")\n",
    "\n",
    "# Play back the two audio to further verify synchronization:\n",
    "print(\"\\033[1mLSL-Synced Audio:\\033[0m\")\n",
    "display(Audio(lsl_audio, rate=sr_lsl))\n",
    "print(\"\\033[1mAligned Video Audio:\\033[0m\")\n",
    "display(Audio(video_audio_aligned, rate=sr_ext_downsampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd8c792-7cd9-4c7d-8272-d754f81f8860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the extracted_audio_aligned after verifying the synchronization:\n",
    "write(aligned_video_audio, sr_ext_downsampled, video_audio_aligned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d3d51b-f0bc-4ad6-84b6-3beb363f8fea",
   "metadata": {},
   "source": [
    "## Aligning the External Video to LSL Timeline\n",
    "\n",
    "After extracting the audio from the external video, we aligned it with the LSL-synced audio using **Shign**. Since both recordings captured the same event but were not synchronized, ```shign_align()``` function detected their timing mismatch and computed the **\"shift_ms\"** needed to align them.\n",
    "\n",
    "Using this precise time shift, we first adjusted the video's audio to match the LSL-synced audio. Now, we apply the same **\"shift_ms\"** value to **trim the external video** itself, ensuring that it is perfectly synchronized with the **LSL timeline**.\n",
    "\n",
    "To do this, we use **```ms_to_samples()```** from Shign, which converts **milliseconds into audio samples**. This conversion allows us to locate the start and end samples in the aligned video audio, which we then use to compute the corresponding **video timestamps (in seconds)** for trimming it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab035106-0c0c-4587-bd06-ef6d3f6b3f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths:\n",
    "video_file = \"external_video/external_video.mp4\"         # External video to be aligned\n",
    "aligned_video = \"outputs/aligned_video.mp4\"              # Path to save the aligned output video\n",
    "\n",
    "# Start sample computed from shift_ms using ms_to_samples():\n",
    "start_sample = ms_to_samples(abs(shift_ms), sr=sr_ext_downsampled)\n",
    "print(f\"Aligned video audio starts at: \\033[1m{abs(shift_ms):.2f}\\033[0m milliseconds\")\n",
    "print(f\"Aligned video audio starts at: \\033[1m{start_sample:.2f}\\033[0m sample\")\n",
    "\n",
    "# End sample computed from start sample + total samples:\n",
    "total_samples = len(video_audio_aligned)\n",
    "end_sample = start_sample + total_samples\n",
    "print(f\"Aligned video audio ends at: \\033[1m{end_sample:.2f}\\033[0m sample\\n\")\n",
    "\n",
    "# Converting audio samples to video timestamps (seconds) using the sample rate of the audio:\n",
    "start_time_sec = start_sample / sr_ext_downsampled\n",
    "end_time_sec = end_sample / sr_ext_downsampled\n",
    "print(f\"Aligned video starts at: \\033[1m{start_time_sec:.2f}\\033[0m second\")\n",
    "print(f\"Aligned video ends at: \\033[1m{end_time_sec:.2f}\\033[0m second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4f7841-563e-4355-9ba8-88495af27e26",
   "metadata": {},
   "source": [
    "---\n",
    "Once we compute the ```start_time_sec``` and ```end_time_sec``` values, we can now trim the video using **FFmpeg** to align it with the LSL-synced audio.\n",
    "\n",
    "**This process may take a while depending on the total video duration and your computer's specifications...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0a740a-ef18-4ae1-b941-de0307c13834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trimming and saving the video using FFmpeg:\n",
    "print(f\"Trimming the video from \\033[1m{start_time_sec:.2f}s\\033[0m to \\033[1m{end_time_sec:.2f}s\\033[0m...\")\n",
    "total_length_sec = end_time_sec-start_time_sec\n",
    "minutes, seconds = divmod(total_length_sec, 60)\n",
    "print(f\"Aligned video length will be \\033[1m{int(minutes)} minutes {int(seconds)} seconds\\033[0m...\")\n",
    "\n",
    "# Setting up the FFmpeg command:\n",
    "command = [\n",
    "    \"ffmpeg\",\n",
    "    \"-ss\", f\"{start_time_sec:.3f}\",                    # start time BEFORE -i ensures frame accuracy\n",
    "    \"-i\", video_file,                                  # input video\n",
    "    \"-to\", f\"{end_time_sec - start_time_sec:.3f}\",     # duration after start\n",
    "    \"-c:v\", \"libx264\",                                 # re-encode video for frame accuracy\n",
    "    \"-c:a\", \"aac\",                     \n",
    "    \"-preset\", \"fast\",                                 # encoding speed optimization\n",
    "    \"-reset_timestamps\", \"1\",                          # reset timestamps after cutting (necessary)\n",
    "    aligned_video                                      # output video path\n",
    "]\n",
    "\n",
    "# Starting the subprocess:\n",
    "process = subprocess.Popen(command, stderr=subprocess.PIPE, stdout=subprocess.PIPE, text=True)\n",
    "\n",
    "# Regular expression to find the time from FFmpeg outputs:\n",
    "time_pattern = re.compile(r\"time=(\\d+:\\d+:\\d+\\.\\d+)\")\n",
    "\n",
    "start = time.time()  # Recording the start time for estimated time calculations\n",
    "\n",
    "# Calculating the estimated time left:\n",
    "try:\n",
    "    calculating_now = False\n",
    "    while True:\n",
    "        line = process.stderr.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        # Find the current time from ffmpeg output:\n",
    "        match = time_pattern.search(line)\n",
    "        if match:\n",
    "            current_time = match.group(1)\n",
    "            h, m, s = map(float, current_time.split(':')) # hour, minute, seconds\n",
    "            elapsed_seconds = h * 3600 + m * 60 + s - start_time_sec\n",
    "            if elapsed_seconds > 30:  # Wait 30s before printing the first estimation\n",
    "                elapsed_time = time.time() - start\n",
    "                # Estimated total time:\n",
    "                estimated_total_time = (elapsed_time / elapsed_seconds) * (end_time_sec - start_time_sec)\n",
    "                # Estimated time left:\n",
    "                etl = estimated_total_time - elapsed_time\n",
    "                # Converting seconds to a more readable format:\n",
    "                eta_hours = int(etl // 3600)\n",
    "                eta_minutes = int((etl % 3600) // 60)\n",
    "                eta_seconds = int(etl % 60)\n",
    "                # Print and update the estimated time left on the same line:\n",
    "                print(f\"Estimated time remaining: {eta_hours}h {eta_minutes}m {eta_seconds}s\", end='\\r')\n",
    "        elif not calculating_now:\n",
    "            print(\"Calculating remaining time...\", end='\\r')\n",
    "            calculating_now = True\n",
    "finally:\n",
    "    process.stderr.close()\n",
    "\n",
    "print(f\"Trimming completed! Aligned video saved in \\033[1m{aligned_video}\\033[0m\\n\")\n",
    "\n",
    "# We can now play the aligned_video for verification:\n",
    "HTML(f\"\"\"\n",
    "<video width=\"1000\" controls>\n",
    "  <source src=\"{aligned_video}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f21d5f-787a-4dca-b6d0-d49b39c5d50d",
   "metadata": {},
   "source": [
    "## Computing Trial Start and End Times from LSL-Synced Raw Audio Files\n",
    "\n",
    "Once the video is also aligned, we determine the **trial start and end times** using LSL timestamps from **trial-sized raw LSL-synced audio files**. Each of these CSV files, stored in the **csv_files** folder, contain LSL timestamps and audio amplitude values for a **single trial**.\n",
    "\n",
    "*(Alternatively, trial timestamps can be extracted from an LSL event marker file).*\n",
    "\n",
    "To map these LSL times to video timestamps, we:\n",
    "\n",
    "- 1. Extract **trial start and end times** (first and last LSL timestamp in each CSV file).\n",
    "- 2. Save them in a **trial_times.csv** file.\n",
    "- 3. Convert these timestamps into **audio sample positions**.\n",
    "- 4. Compute the **video timestamps (seconds)** using the audio **sample rate**.\n",
    "\n",
    "Once these steps are completed, we can use the computed video timestamps to segment the aligned video into **trial-sized clips**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0cea4a-1b34-44d4-8236-00ec8d923836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing the raw LSL-synced audio files of all trials:\n",
    "csv_folder = \"csv_files\"\n",
    "\n",
    "results = []\n",
    "\n",
    "# Iterating over all CSV files in the folder:\n",
    "for file_name in os.listdir(csv_folder):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        file_path = os.path.join(csv_folder, file_name)\n",
    "            \n",
    "        # For each file:\n",
    "        try:\n",
    "            # Read the CSV file (assuming two columns: \"time_ms\" and \"amplitude\"):\n",
    "            data = pd.read_csv(file_path, header=None, names=[\"time_ms\", \"amplitude\"])\n",
    "                \n",
    "            # Extract the trial start and end times (first and last timestamps)\n",
    "            start_time = data[\"time_ms\"].iloc[1]\n",
    "            end_time = data[\"time_ms\"].iloc[-1]\n",
    "                \n",
    "            # Store results, including the current filename:\n",
    "            results.append({\n",
    "                \"file_name\": file_name,\n",
    "                \"start_time_ms\": start_time,\n",
    "                \"end_time_ms\": end_time\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_name}: {e}\")\n",
    "    \n",
    "# Converting results to DataFrame and saving:\n",
    "trial_times_df = pd.DataFrame(results)\n",
    "save_dir = \"outputs/trial_times.csv\"\n",
    "trial_times_df.to_csv(save_dir, index=False)\n",
    "\n",
    "# Printing the first 10 rows to inspect the accuracy of the saved times:\n",
    "trial_times_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f3e2e8-863c-4751-9e78-4083d76da89a",
   "metadata": {},
   "source": [
    "Now, we can map these **trial times** to **audio sample positions** and **video timestamps**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fed062e-b0c2-4eb8-83ed-07adb223e4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths:\n",
    "csv_file = \"lsl_synced_audio/lsl_synced_long_audio_raw.csv\"     # Raw LSL-synced audio in CSV format\n",
    "mapped_output_file = \"outputs/mapped_event_markers.csv\"         # Path to save the mapped timestamps\n",
    "\n",
    "# Sample rate of LSL-synced audio:\n",
    "lsl_audio_sr = sr_lsl\n",
    "\n",
    "# Loading files:\n",
    "lsl_audio_raw = pd.read_csv(csv_file, skiprows=1, header=None, names=[\"time_ms\", \"value\"])       # Renaming the columns\n",
    "\n",
    "# Adding new columns to the trial_times_df for the mapped audio sample positions and video timestamps:\n",
    "\n",
    "trial_times_df[\"lsl_audio_start_sample\"] = None             # Sample number in LSL-synced audio for start\n",
    "trial_times_df[\"lsl_audio_end_sample\"] = None               # Sample number in LSL-synced audio for end\n",
    "trial_times_df[\"video_start_time\"] = None                   # Time in video (seconds) for start\n",
    "trial_times_df[\"video_end_time\"] = None                     # Time in video (seconds) for end\n",
    "\n",
    "# Iterating over all rows of the trial_times_df:\n",
    "for idx, row in trial_times_df.iterrows():\n",
    "    \n",
    "    # Calculating lsl_audio_start_sample by counting rows up to start_time_ms in lsl_audio_raw:\n",
    "    lsl_audio_start_sample = lsl_audio_raw[lsl_audio_raw[\"time_ms\"] <= row[\"start_time_ms\"]].shape[0]\n",
    "    \n",
    "    # Calculate lsl_audio_end_sample by counting rows up to end_time_ms in lsl_audio_raw:\n",
    "    lsl_audio_end_sample = lsl_audio_raw[lsl_audio_raw[\"time_ms\"] <= row[\"end_time_ms\"]].shape[0]\n",
    "\n",
    "    # Calculate video_start_time and video_end_time (in seconds) based on the sample rate of the LSL-synced audio:\n",
    "    video_start_time = lsl_audio_start_sample / lsl_audio_sr\n",
    "    video_end_time = lsl_audio_end_sample / lsl_audio_sr\n",
    "\n",
    "    # Updating the DataFrame:\n",
    "    trial_times_df.at[idx, \"lsl_audio_start_sample\"] = lsl_audio_start_sample\n",
    "    trial_times_df.at[idx, \"lsl_audio_end_sample\"] = lsl_audio_end_sample\n",
    "    trial_times_df.at[idx, \"video_start_time\"] = round(video_start_time, 6)\n",
    "    trial_times_df.at[idx, \"video_end_time\"] = round(video_end_time, 6)\n",
    "\n",
    "# Saving the updated DataFrame including the computed video timestamps:\n",
    "trial_times_df.to_csv(mapped_output_file, index=False)\n",
    "\n",
    "# Inspecting the first 10 rows:\n",
    "trial_times_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba581cb-8056-4e2c-a8b1-740db7c6ee9e",
   "metadata": {},
   "source": [
    "## Segmenting the Aligned Video into Trial-Sized Clips\n",
    "\n",
    "Now, we computed the video start and end times for each trial.\n",
    "\n",
    "Using these timestamps, we can **segment the aligned video** into **trial-sized clips**.\n",
    "\n",
    "**Segmenting trial clips may take a while, depending on video length, trial count, and your computer's specifications...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63337d03-e2f6-40fa-9620-3b702c59db39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# File paths:\n",
    "input_video = \"outputs/aligned_video.mp4\"\n",
    "output_folder = \"outputs/cut_videos/\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for i, row in trial_times_df.iterrows():\n",
    "    start_time = row[\"video_start_time\"]\n",
    "    end_time = row[\"video_end_time\"]\n",
    "    file_name = row[\"file_name\"]\n",
    "    \n",
    "    # Creating an output filename based on the input filename (.mp4 instead of .csv):\n",
    "    output_file = os.path.join(output_folder, f\"{file_name.replace('.csv', '')}.mp4\")\n",
    "    \n",
    "    # Using FFmpeg to segment the videos - without audio:\n",
    "    ffmpeg_command = [\n",
    "    \"ffmpeg\",\n",
    "    \"-ss\", f\"{start_time:.3f}\",         \n",
    "    \"-i\", input_video,                  \n",
    "    \"-to\", f\"{end_time - start_time:.3f}\", \n",
    "    \"-c:v\", \"libx264\",                  \n",
    "    \"-an\",                              # Disable audio (replace with \"-c:a\", \"aac\", if audio is needed)\n",
    "    \"-preset\", \"fast\",                  # Faster encoding\n",
    "    \"-reset_timestamps\", \"1\",           # Reset timestamps\n",
    "    \"-filter_complex\", \n",
    "    \"[0:v]setpts=PTS-STARTPTS[v]\",      # Reset video PTS\n",
    "    \"-map\", \"[v]\",                      # Map video stream\n",
    "    \"-movflags\", \"+faststart\",          # Optimize for playback\n",
    "    output_file\n",
    "    ]\n",
    "    \n",
    "    # Execute the command using subprocess:\n",
    "    subprocess.run(ffmpeg_command, check=True)\n",
    "    print(f\"Segment saved in {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e9ba75-95cd-42ca-96f6-8dffa5b074ca",
   "metadata": {},
   "source": [
    "## (Optional) Overlaying the Audio of Each Trial onto Their Cut Videos\n",
    "\n",
    "After segmenting the trial videos without the audio stream, we can optionally **overlay the corresponding LSL audio** to each trial clip for verification or further analysis.\n",
    "\n",
    "**Note:** This step assumes that all raw trial audios in csv_files folder have already been converted to WAV files and are stored in the **```audio_files/```** folder.\n",
    "\n",
    "To ensure correct pairing of audio and video of trials, the following chunk:\n",
    "\n",
    "- **Sorts video files** from the trial video_folder.\n",
    "- **Sorts audio files** from the audio_folder.\n",
    "- Matches video and audio files **based on their filenames**.\n",
    "- Uses **FFmpeg** to merge the video with its corresponding audio.\n",
    "- Saves the new audio-overlaid trial videos in the **```outputs/audio_overlay/```** folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed8be65-e5a2-4597-8258-67d6ce542080",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# File paths:\n",
    "video_folder = \"outputs/cut_videos/\"          # folder with all the trial videos\n",
    "audio_folder = \"audio_files/\"                 # folder with all the trial audio files (names should match)\n",
    "output_folder = \"outputs/audio_overlay/\"      # output folder to save the audio-overlaid videos\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Getting sorted lists of video and audio files:\n",
    "video_files = sorted([f for f in os.listdir(video_folder) if f.endswith(\".mp4\")])\n",
    "audio_files = sorted([f for f in os.listdir(audio_folder) if f.endswith(\".wav\")])\n",
    "\n",
    "for video_file, audio_file in zip(video_files, audio_files):\n",
    "    \n",
    "    # Checking again if the first 10 characters of the filenames match:\n",
    "    if video_file[:10] == audio_file[:10]:\n",
    "        \n",
    "        video_path = os.path.join(video_folder, video_file)\n",
    "        audio_path = os.path.join(audio_folder, audio_file)\n",
    "        \n",
    "        # Saving the audio-pverlaid video with same name as the original trial video:\n",
    "        output_path = os.path.join(output_folder, video_file)\n",
    "        \n",
    "        # FFmpeg command:\n",
    "        ffmpeg_command = [\n",
    "            \"ffmpeg\",\n",
    "            \"-i\", video_path,    \n",
    "            \"-i\", audio_path,     \n",
    "            \"-c:v\", \"copy\",       \n",
    "            \"-c:a\", \"aac\",        \n",
    "            \"-map\", \"0:v:0\",      # Map the first input's video\n",
    "            \"-map\", \"1:a:0\",      # Map the second input's audio\n",
    "            output_path           \n",
    "        ]\n",
    "        \n",
    "        # Running the FFmpeg command:\n",
    "        subprocess.run(ffmpeg_command, check=True, capture_output=True)\n",
    "        print(f\"Audio-overlaid video saved in {output_path}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"No matching audio found for the video: {video_file}\")\n",
    "\n",
    "print(\"All videos processed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec42421d-76b1-4fb2-9bc7-bf58bb56b053",
   "metadata": {},
   "source": [
    "## Example Segmented Trial Videos with Overlaid Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d322b26-006d-4fd9-b1ad-1133d2147dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path:\n",
    "video_path_1 = \"outputs/audio_overlay/0_1_trial_12_Mic_nominal_srate16000_p1_ei_geluiden.mp4\"\n",
    "\n",
    "HTML(f\"\"\"\n",
    "<video width=\"1000\" controls>\n",
    "  <source src=\"{video_path_1}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d43b762-8bf0-4ab4-98bd-cb673cfba4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path:\n",
    "video_path_2 = \"outputs/audio_overlay/0_1_pr_1_Mic_nominal_srate16000_p0_dansen_geluiden_corrected.mp4\"\n",
    "\n",
    "HTML(f\"\"\"\n",
    "<video width=\"1000\" controls>\n",
    "  <source src=\"{video_path_2}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
